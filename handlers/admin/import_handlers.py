# epicservice/handlers/admin/import_handlers.py

import asyncio
import logging
import os
import shutil
from datetime import datetime

import pandas as pd
from aiogram import Bot, F, Router
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.types import FSInputFile, Message, CallbackQuery, InlineKeyboardMarkup, InlineKeyboardButton
from sqlalchemy import select, update, func

from config import ADMIN_IDS, ARCHIVES_PATH, BACKUP_DIR, DB_NAME, DB_TYPE
from database.engine import async_session
from database.models import Product, StockHistory
from keyboards.reply import get_admin_menu_kb
from keyboards.inline import get_yes_no_kb
from utils.import_processor import generate_import_preview, process_import_dataframe, read_excel_smart
from utils.markdown_corrector import format_filename_safe, escape_markdown

# ğŸ‘‡ Ğ†Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ÑƒÑ”Ğ¼Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ¸
from constants import DEPARTMENTS, DEPARTMENT_EMOJIS

logger = logging.getLogger(__name__)
router = Router()

class ImportStates(StatesGroup):
    waiting_for_file = State()
    confirming_preview = State()

def get_cancel_kb() -> InlineKeyboardMarkup:
    return InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="âŒ Ğ¡ĞºĞ°ÑÑƒĞ²Ğ°Ñ‚Ğ¸", callback_data="import:cancel_early")]
    ])

# ==============================================================================
# ğŸ’¾ ĞĞ’Ğ¢ĞĞœĞĞ¢Ğ˜Ğ§ĞĞ˜Ğ™ Ğ‘Ğ•ĞšĞĞŸ
# ==============================================================================

async def create_backup_before_import() -> bool:
    try:
        if DB_TYPE == "sqlite":
            os.makedirs(BACKUP_DIR, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"auto_backup_before_import_{timestamp}.db"
            backup_path = os.path.join(BACKUP_DIR, backup_filename)
            
            if os.path.exists(DB_NAME):
                shutil.copy2(DB_NAME, backup_path)
                logger.info("ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ğ±ĞµĞºĞ°Ğ¿ ÑÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¾: %s", backup_filename)
                return True
        return True
    except Exception as e:
        logger.error("ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° ÑÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ğ±ĞµĞºĞ°Ğ¿Ñƒ: %s", e, exc_info=True)
        return False

# ==============================================================================
# ğŸ“¥ ĞŸĞĞ§ĞĞ¢ĞĞš Ğ†ĞœĞŸĞĞ Ğ¢Ğ£
# ==============================================================================

async def proceed_with_import(message: Message, state: FSMContext, bot: Bot):
    if message.from_user.id not in ADMIN_IDS: return

    await state.set_state(ImportStates.waiting_for_file)
    await message.answer(
        "ğŸ“¥ **Ğ†Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑ–Ğ²**\n\n"
        "ĞĞ°Ğ´Ñ–ÑˆĞ»Ñ–Ñ‚ÑŒ Excel Ñ„Ğ°Ğ¹Ğ».\n"
        "â€¢ ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»Ğ¸ Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ±ÑƒĞ´ÑƒÑ‚ÑŒ Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ–/Ğ´Ğ¾Ğ´Ğ°Ğ½Ñ–.\n"
        "â€¢ ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»Ğ¸, ÑĞºĞ¸Ñ… ĞĞ•ĞœĞĞ„ Ğ² Ñ„Ğ°Ğ¹Ğ»Ñ–, Ğ±ÑƒĞ´ÑƒÑ‚ÑŒ Ğ´ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ²Ğ°Ğ½Ñ–.\n\n"
        "ğŸ‘‡ Ğ”Ğ»Ñ ÑĞºĞ°ÑÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ½Ğ°Ñ‚Ğ¸ÑĞ½Ñ–Ñ‚ÑŒ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ:",
        reply_markup=get_cancel_kb()
    )

@router.callback_query(F.data == "import:cancel_early")
async def cancel_import_early(callback: CallbackQuery, state: FSMContext):
    await state.clear()
    await callback.message.delete()
    await callback.message.answer("âŒ Ğ†Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ ÑĞºĞ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¾.", reply_markup=get_admin_menu_kb())
    await callback.answer()

# ==============================================================================
# ğŸ“„ ĞĞ‘Ğ ĞĞ‘ĞšĞ Ğ¤ĞĞ™Ğ›Ğ£
# ==============================================================================

@router.message(ImportStates.waiting_for_file, F.document)
async def process_import_file_with_preview(message: Message, state: FSMContext, bot: Bot):
    if message.from_user.id not in ADMIN_IDS: return

    document = message.document
    if not document.file_name.lower().endswith((".xlsx", ".xls", ".ods")):
        await message.answer("âŒ ĞĞµĞ²Ñ–Ñ€Ğ½Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ„Ğ°Ğ¹Ğ»Ñƒ. ĞŸĞ¾Ñ‚Ñ€Ñ–Ğ±ĞµĞ½ Excel.")
        return

    msg = await message.answer("â³ ĞĞ½Ğ°Ğ»Ñ–Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ (Smart Read)...")

    try:
        file = await bot.get_file(document.file_id)
        file_path = os.path.join(
            ARCHIVES_PATH,
            f"import_temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        )
        os.makedirs(ARCHIVES_PATH, exist_ok=True)
        await bot.download_file(file.file_path, file_path)

        loop = asyncio.get_running_loop()
        df, header_row_idx = await loop.run_in_executor(None, read_excel_smart, file_path)
        preview = generate_import_preview(df)

        await state.update_data(
            file_path=file_path,
            filename=document.file_name,
            total_rows=len(df),
            header_row_idx=header_row_idx
        )
        await state.set_state(ImportStates.confirming_preview)

        preview_text = (
            "ğŸ‘ **ĞŸĞ Ğ•Ğ’Ê¼Ğ® Ğ†ĞœĞŸĞĞ Ğ¢Ğ£**\n\n"
            f"ğŸ“„ Ğ¤Ğ°Ğ¹Ğ»: `{format_filename_safe(document.file_name)}`\n"
            f"ğŸ“Œ Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ° Ñ€ÑĞ´ĞºÑƒ: **{header_row_idx + 1}**\n"
            f"ğŸ“Š Ğ ÑĞ´ĞºÑ–Ğ² Ğ´Ğ°Ğ½Ğ¸Ñ…: {preview.stats['total_rows']}\n"
            f"ğŸ“‹ ĞšĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº: {preview.stats['columns_count']}\n\n"
            "ğŸ” **Ğ Ğ¾Ğ·Ğ¿Ñ–Ğ·Ğ½Ğ°Ğ½Ñ– ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸:**\n"
        )

        for standard, detected in preview.columns_detected.items():
            emoji = "âœ…" if detected else "âŒ"
            std_names = {
                "department": "Ğ’Ñ–Ğ´Ğ´Ñ–Ğ»", "group": "Ğ“Ñ€ÑƒĞ¿Ğ°", "article": "ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»",
                "name": "ĞĞ°Ğ·Ğ²Ğ°", "quantity": "ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ", "sum": "Ğ¡ÑƒĞ¼Ğ°", "months_no_movement": "Ğ‘ĞµĞ· Ñ€ÑƒÑ…Ñƒ"
            }
            std_name = std_names.get(standard, standard)
            det_safe = escape_markdown(detected) if detected else 'Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾'
            preview_text += f"{emoji} {std_name}: {det_safe}\n"

        if preview.unknown_columns:
            preview_text += f"\nâ“ ĞĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ñ– ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {len(preview.unknown_columns)} (Ğ±ÑƒĞ´ÑƒÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ–Ğ³Ğ½Ğ¾Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ–)"

        preview_text += "\n\nâš ï¸ **ĞŸÑ–Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ñ–Ñ‚ÑŒ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚:**"

        await msg.delete()
        await message.answer(
            preview_text, 
            reply_markup=get_yes_no_kb("import"),
            parse_mode="Markdown"
        )

    except Exception as e:
        logger.error("ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ: %s", e, exc_info=True)
        try: await msg.delete()
        except: pass
        await message.answer(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ Ñ„Ğ°Ğ¹Ğ»Ñƒ:\n{str(e)[:200]}")
        if "file_path" in locals() and os.path.exists(file_path): os.remove(file_path)
        await state.clear()

# ==============================================================================
# âœ… ĞŸĞ†Ğ”Ğ¢Ğ’Ğ•Ğ Ğ”Ğ–Ğ•ĞĞĞ¯ Ğ¢Ğ Ğ—Ğ’Ğ†Ğ¢
# ==============================================================================

@router.callback_query(ImportStates.confirming_preview, F.data == "confirm:import:yes")
async def confirm_and_import(callback: CallbackQuery, state: FSMContext):
    await callback.message.delete()
    data = await state.get_data()
    file_path = data.get("file_path")
    filename = data.get("filename")
    
    if not file_path or not os.path.exists(file_path):
        await callback.message.answer("âŒ Ğ¤Ğ°Ğ¹Ğ» Ğ²Ñ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ¾.")
        await state.clear()
        return

    msg = await callback.message.answer("ğŸ’¾ Ğ‘ĞµĞºĞ°Ğ¿...")
    await create_backup_before_import()
    await msg.edit_text("ğŸ“Š Ğ¡Ğ˜ĞĞ¥Ğ ĞĞĞ†Ğ—ĞĞ¦Ğ†Ğ¯ Ğ‘ĞĞ—Ğ˜...")
    
    try:
        loop = asyncio.get_running_loop()
        df, _ = await loop.run_in_executor(None, read_excel_smart, file_path)
        processed_df, validation = await loop.run_in_executor(
            None, process_import_dataframe, df, None
        )

        if not validation.is_valid:
            error_text = "\n".join(validation.errors[:10])
            await msg.edit_text(f"âŒ Ğ’Ğ°Ğ»Ñ–Ğ´Ğ°Ñ†Ñ–Ñ Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ¹ÑˆĞ»Ğ°!\n\n{error_text}")
            if os.path.exists(file_path): os.remove(file_path)
            await state.clear()
            return

        # --- Ğ†ĞœĞŸĞĞ Ğ¢ ---
        added, updated, deactivated, reactivated, zero_qty = 0, 0, 0, 0, 0
        file_articles = set()

        async with async_session() as session:
            # ĞÑ‚Ñ€Ğ¸Ğ¼ÑƒÑ”Ğ¼Ğ¾ Ğ²ÑÑ– Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ– Ğ· Ğ‘Ğ”
            res_all = await session.execute(select(Product.Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»).where(Product.Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹ == True))
            db_active_articles = set(res_all.scalars().all())

            for _, row in processed_df.iterrows():
                try:
                    art = str(row["Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»"])
                    file_articles.add(art)
                    qty_str = str(row["ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ"]).replace('.', ',')
                    price_float = float(row["Ñ†Ñ–Ğ½Ğ°"]) if row["Ñ†Ñ–Ğ½Ğ°"] is not None else 0.0
                    
                    try:
                        if float(str(row["ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ"]).replace(",", ".")) == 0: zero_qty += 1
                    except: pass

                    res = await session.execute(select(Product).where(Product.Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ» == art))
                    existing = res.scalar_one_or_none()

                    if existing:
                        if not existing.Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹:
                            existing.Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹ = True
                            reactivated += 1
                        
                        if existing.ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ != qty_str:
                            hist = StockHistory(
                                product_id=existing.id, articul=art,
                                old_quantity=existing.ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ, new_quantity=qty_str,
                                change_source="import"
                            )
                            session.add(hist)
                        
                        existing.ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ = qty_str
                        
                        # Ğ§Ğ°ÑÑ‚ĞºĞ¾Ğ²Ğµ Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ Ğ¿Ğ¾Ğ»Ñ–Ğ²
                        if row["Ñ†Ñ–Ğ½Ğ°"] is not None: existing.Ñ†Ñ–Ğ½Ğ° = price_float
                        if row["ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ"] is not None: existing.ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ = float(row["ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ"])
                        if row["Ğ¼Ñ–ÑÑÑ†Ñ–_Ğ±ĞµĞ·_Ñ€ÑƒÑ…Ñƒ"] is not None: existing.Ğ¼Ñ–ÑÑÑ†Ñ–_Ğ±ĞµĞ·_Ñ€ÑƒÑ…Ñƒ = int(row["Ğ¼Ñ–ÑÑÑ†Ñ–_Ğ±ĞµĞ·_Ñ€ÑƒÑ…Ñƒ"])
                        if row["Ğ½Ğ°Ğ·Ğ²Ğ°"]: existing.Ğ½Ğ°Ğ·Ğ²Ğ° = row["Ğ½Ğ°Ğ·Ğ²Ğ°"]
                        if row["Ğ³Ñ€ÑƒĞ¿Ğ°"]: existing.Ğ³Ñ€ÑƒĞ¿Ğ° = row["Ğ³Ñ€ÑƒĞ¿Ğ°"]
                        if row["Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»"]: existing.Ğ²Ñ–Ğ´Ğ´Ñ–Ğ» = row["Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»"]
                        
                        updated += 1
                    else:
                        new_p = Product(
                            Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»=art, Ğ½Ğ°Ğ·Ğ²Ğ°=row["Ğ½Ğ°Ğ·Ğ²Ğ°"] or "Ğ‘ĞµĞ· Ğ½Ğ°Ğ·Ğ²Ğ¸", 
                            Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»=row["Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»"] or 0, Ğ³Ñ€ÑƒĞ¿Ğ°=row["Ğ³Ñ€ÑƒĞ¿Ğ°"] or "",
                            ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ=qty_str, Ñ†Ñ–Ğ½Ğ°=price_float,
                            ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ=float(row["ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ"]) if row["ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ"] else 0.0,
                            Ğ¼Ñ–ÑÑÑ†Ñ–_Ğ±ĞµĞ·_Ñ€ÑƒÑ…Ñƒ=int(row["Ğ¼Ñ–ÑÑÑ†Ñ–_Ğ±ĞµĞ·_Ñ€ÑƒÑ…Ñƒ"]) if row["Ğ¼Ñ–ÑÑÑ†Ñ–_Ğ±ĞµĞ·_Ñ€ÑƒÑ…Ñƒ"] else 0,
                            Ğ²Ñ–Ğ´ĞºĞ»Ğ°Ğ´ĞµĞ½Ğ¾=0, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹=True
                        )
                        session.add(new_p)
                        added += 1
                except: pass

            # Ğ”ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ñ–Ñ
            to_deact = db_active_articles - file_articles
            if to_deact:
                await session.execute(
                    update(Product).where(Product.Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ».in_(to_deact)).values(Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹=False)
                )
                deactivated = len(to_deact)

            await session.commit()

            # --- ğŸ“Š Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ†Ğ¯ Ğ—Ğ’Ğ•Ğ”Ğ•ĞĞĞ“Ğ Ğ—Ğ’Ğ†Ğ¢Ğ£ ĞŸĞ Ğ¡ĞšĞ›ĞĞ”Ğ£ ---
            
            # Ğ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ñ– Ğ¿Ğ¾ĞºĞ°Ğ·Ğ½Ğ¸ĞºĞ¸
            total_items_query = await session.execute(
                select(func.count(Product.id)).where(Product.Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹ == True)
            )
            total_items = total_items_query.scalar_one()

            total_value_query = await session.execute(
                select(func.sum(Product.ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ)).where(Product.Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹ == True)
            )
            total_value = total_value_query.scalar_one() or 0.0

            # ĞŸĞ¾ Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»Ğ°Ñ…
            dept_stats_query = await session.execute(
                select(
                    Product.Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»,
                    func.count(Product.id),
                    func.sum(Product.ÑÑƒĞ¼Ğ°_Ğ·Ğ°Ğ»Ğ¸ÑˆĞºÑƒ)
                )
                .where(Product.Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹ == True)
                .group_by(Product.Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»)
                .order_by(Product.Ğ²Ñ–Ğ´Ğ´Ñ–Ğ»)
            )
            dept_stats = dept_stats_query.all()

        if os.path.exists(file_path): os.remove(file_path)

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ñ‡Ğ¸ÑĞµĞ»
        def fmt(num):
            return f"{num:,.0f}".replace(",", " ")

        # --- Ğ¡Ğ¢Ğ’ĞĞ Ğ•ĞĞĞ¯ ĞšĞ ĞĞ¡Ğ˜Ğ’ĞĞ“Ğ Ğ—Ğ’Ğ†Ğ¢Ğ£ ---
        report_text = (
            f"âœ… **Ğ¡Ğ˜ĞĞ¥Ğ ĞĞĞ†Ğ—ĞĞ¦Ğ†Ğ® Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ!**\n"
            f"ğŸ“„ Ğ¤Ğ°Ğ¹Ğ»: `{format_filename_safe(filename)}`\n\n"
            f"â• Ğ”Ğ¾Ğ´Ğ°Ğ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¸Ñ…: {added}\n"
            f"ğŸ”„ ĞĞ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾: {updated - reactivated}\n"
            f"â™»ï¸ Ğ’Ñ–Ğ´Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾: {reactivated}\n"
            f"ğŸ”´ Ğ”ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¾: {deactivated}\n"
            f"âš ï¸ ĞÑƒĞ»ÑŒĞ¾Ğ²Ğ¸Ñ…: {zero_qty}\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            f"ğŸ“Š **Ğ¡Ğ¢ĞĞ Ğ¡ĞšĞ›ĞĞ”Ğ£**\n\n"
            f"Ğ’ÑÑŒĞ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ²: **{fmt(total_items)}** (Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ–)\n"
            f"Ğ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ° Ğ²Ğ°Ñ€Ñ‚Ñ–ÑÑ‚ÑŒ: **{fmt(total_value)} Ğ³Ñ€Ğ½**\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            "ğŸ“ **ĞŸĞ Ğ’Ğ†Ğ”Ğ”Ğ†Ğ›ĞĞ¥:**\n\n"
        )

        for dept_code, count, value in dept_stats:
            val = value or 0
            # ĞÑ‚Ñ€Ğ¸Ğ¼ÑƒÑ”Ğ¼Ğ¾ Ğ½Ğ°Ğ·Ğ²Ñƒ Ñ‚Ğ° ĞµĞ¼Ğ¾Ğ´Ğ·Ñ– Ğ· ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚
            dept_name = DEPARTMENTS.get(dept_code, str(dept_code))
            emoji = DEPARTMENT_EMOJIS.get(dept_code, "ğŸ“¦")
            
            # Ğ¯ĞºÑ‰Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ° Ğ´Ğ¾Ğ²Ğ³Ğ°, Ğ¾Ğ±Ñ€Ñ–Ğ·Ğ°Ñ”Ğ¼Ğ¾ Ğ°Ğ±Ğ¾ ÑĞºĞ¾Ñ€Ğ¾Ñ‡ÑƒÑ”Ğ¼Ğ¾ Ğ´Ğ»Ñ ĞºÑ€Ğ°ÑĞ¸
            if len(dept_name) > 20: dept_name = dept_name[:19] + "â€¦"

            report_text += f"{emoji} **{dept_name}**\n   â”” {count} Ğ°Ñ€Ñ‚. | **{fmt(val)} Ğ³Ñ€Ğ½**\n"

        report_text += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

        await msg.edit_text(report_text)
        await state.clear()
        await callback.answer()

    except Exception as e:
        logger.error("Import error: %s", e, exc_info=True)
        await msg.delete()
        await callback.message.answer(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ°: {e}")
        if os.path.exists(file_path): os.remove(file_path)
        await state.clear()
        await callback.answer()

@router.callback_query(ImportStates.confirming_preview, F.data == "confirm:import:no")
async def cancel_import(callback: CallbackQuery, state: FSMContext):
    data = await state.get_data()
    path = data.get("file_path")
    if path and os.path.exists(path): os.remove(path)
    
    await callback.message.delete()
    await callback.message.answer("âŒ Ğ†Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ ÑĞºĞ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¾.", reply_markup=get_admin_menu_kb())
    await state.clear()
    await callback.answer()

@router.message(ImportStates.waiting_for_file)
async def invalid_import_file(message: Message):
    await message.answer("âŒ ĞĞ°Ğ´Ñ–ÑˆĞ»Ñ–Ñ‚ÑŒ Excel Ñ„Ğ°Ğ¹Ğ» Ğ°Ğ±Ğ¾ Ğ½Ğ°Ñ‚Ğ¸ÑĞ½Ñ–Ñ‚ÑŒ Ğ¡ĞºĞ°ÑÑƒĞ²Ğ°Ñ‚Ğ¸.")

@router.message(F.text == "ğŸ“¤ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½")
async def download_import_template(message: Message):
    if message.from_user.id not in ADMIN_IDS: return
    await message.answer("Ğ¤ÑƒĞ½ĞºÑ†Ñ–Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñƒ.")