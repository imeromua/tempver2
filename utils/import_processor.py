# epicservice/utils/import_processor.py

import logging
import re
import json
import os
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any

import pandas as pd

logger = logging.getLogger(__name__)
MAPPING_FILE = "column_mapping.json"

# ==============================================================================
# ðŸ“‹ Ð¡Ð›ÐžÐ’ÐÐ˜Ðš ÐšÐžÐ›ÐžÐÐžÐš (Ð‘ÐÐ—ÐžÐ’Ð˜Ð™)
# ==============================================================================

DEFAULT_MAPPING = {
    "department": ["Ð²", "Ð²Ñ–Ð´Ð´Ñ–Ð»", "code", "department", "dept", "Ð¾Ñ‚Ð´ÐµÐ»", "ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ñ–Ñ", "ÐºÐ¾Ð´ Ð²Ñ–Ð´Ð´Ñ–Ð»Ñƒ"],
    "group": ["Ð³", "Ð³Ñ€ÑƒÐ¿Ð°", "group", "fg1_name", "Ð¿Ñ–Ð´Ð³Ñ€ÑƒÐ¿Ð°", "Ð³Ñ€ÑƒÐ¿Ð¿Ð°", "subgroup"],
    "article": ["Ð°", "Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»", "article", "articul", "ÐºÐ¾Ð´", "code_product", "product_code"],
    "name": ["Ð½", "Ð½Ð°Ð·Ð²Ð°", "Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ", "name", "product", "Ñ‚Ð¾Ð²Ð°Ñ€", "Ð½Ð°Ð¹Ð¼ÐµÐ½ÑƒÐ²Ð°Ð½Ð½Ñ", "articul_name"],
    "quantity": ["Ðº", "ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ", "quantity", "qty", "Ð·Ð°Ð»Ð¸ÑˆÐ¾Ðº", "Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ðº", "Ð·Ð°Ð»Ð¸ÑˆÐ¾Ðº, Ðº-Ñ‚ÑŒ", "Ðº-Ñ‚ÑŒ"],
    "sum": ["Ñ", "ÑÑƒÐ¼Ð°", "sum", "ÑÑƒÐ¼Ð¼Ð°", "Ð·Ð°Ð»Ð¸ÑˆÐ¾Ðº, ÑÑƒÐ¼Ð°", "total", "ÑÑƒÐ¼Ð° Ð·Ð°Ð»Ð¸ÑˆÐºÑƒ"],
    "months_no_movement": ["Ð¼", "Ð¼Ñ–ÑÑÑ†Ñ– Ð±ÐµÐ· Ñ€ÑƒÑ…Ñƒ", "Ð¼Ñ–ÑÑÑ†Ñ–Ð² Ð±ÐµÐ· Ñ€ÑƒÑ…Ñƒ", "Ð±ÐµÐ· Ñ€ÑƒÑ…Ñƒ", "months", "no_movement"]
}

# Ð¡Ð¿Ð¸ÑÐ¾Ðº ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº, ÑÐºÑ– Ð¼Ð¸ Ð²Ð¸Ñ€Ñ–ÑˆÐ¸Ð»Ð¸ Ñ–Ð³Ð½Ð¾Ñ€ÑƒÐ²Ð°Ñ‚Ð¸ Ð½Ð°Ð·Ð°Ð²Ð¶Ð´Ð¸
IGNORED_COLUMNS = ["Ñ‚Ñ†", "period_type", "war_status", "simple_name", "Ðº-Ñ‚ÑŒ Ð°Ñ€Ñ‚"]

@dataclass
class ImportValidation:
    is_valid: bool
    errors: List[str]
    warnings: List[str]
    total_rows: int
    valid_rows: int

@dataclass
class ImportPreview:
    columns_detected: Dict[str, str]
    unknown_columns: List[str]
    sample_rows: pd.DataFrame
    stats: Dict[str, Any]
    header_row_index: int

# ==============================================================================
# ðŸ§  Ð ÐžÐ—Ð£ÐœÐÐ• Ð§Ð˜Ð¢ÐÐÐÐ¯ (SMART READ)
# ==============================================================================

def read_excel_smart(file_path: str) -> Tuple[pd.DataFrame, int]:
    """
    Ð—Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÑŽÑ‡Ð¸ ÑÐ¼Ñ–Ñ‚Ñ‚Ñ Ð½Ð° Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÑƒ Ñ„Ð°Ð¹Ð»Ñƒ.
    """
    try:
        # Ð§Ð¸Ñ‚Ð°Ñ”Ð¼Ð¾ Ð¿ÐµÑ€ÑˆÑ– 20 Ñ€ÑÐ´ÐºÑ–Ð²
        preview_df = pd.read_excel(file_path, header=None, nrows=20)
    except Exception as e:
        logger.error(f"Read error: {e}")
        return pd.read_excel(file_path), 0

    best_idx = 0
    max_matches = 0
    
    # Ð—Ð±Ð¸Ñ€Ð°Ñ”Ð¼Ð¾ Ð²ÑÑ– Ð²Ñ–Ð´Ð¾Ð¼Ñ– Ð½Ð°Ð¼ ÑÐ»Ð¾Ð²Ð°
    keywords = set()
    for aliases in DEFAULT_MAPPING.values():
        for a in aliases: keywords.add(a.lower())
    
    # Ð¨ÑƒÐºÐ°Ñ”Ð¼Ð¾ Ñ€ÑÐ´Ð¾Ðº Ð· Ð½Ð°Ð¹Ð±Ñ–Ð»ÑŒÑˆÐ¾ÑŽ ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŽ Ð·Ð½Ð°Ð¹Ð¾Ð¼Ð¸Ñ… ÑÐ»Ñ–Ð²
    for idx, row in preview_df.iterrows():
        matches = 0
        row_vals = [str(v).lower().strip() for v in row.values if pd.notna(v)]
        
        for v in row_vals:
            if v in keywords: matches += 1
        
        if matches > max_matches:
            max_matches = matches
            best_idx = idx

    logger.info(f"Smart Read: Header found at row {best_idx} (matches: {max_matches})")

    # Ð§Ð¸Ñ‚Ð°Ñ”Ð¼Ð¾ Ð½Ð°Ñ‡Ð¸ÑÑ‚Ð¾
    df = pd.read_excel(file_path, header=best_idx)
    # ÐžÑ‡Ð¸Ñ‰Ð°Ñ”Ð¼Ð¾ Ð½Ð°Ð·Ð²Ð¸ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
    df.columns = df.columns.astype(str).str.strip()
    return df, best_idx

# ==============================================================================
# ðŸ’¾ ÐœÐ•ÐÐ•Ð”Ð–Ð•Ð  ÐœÐÐŸÐ†ÐÐ“Ð£ (JSON)
# ==============================================================================

def load_custom_mapping() -> Dict[str, List[str]]:
    """Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑ” Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ñ– Ð½Ð°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡Ð°."""
    if not os.path.exists(MAPPING_FILE):
        return {}
    try:
        with open(MAPPING_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Config load error: {e}")
        return {}

def update_saved_mapping(internal_key: str, file_column_name: str):
    """
    Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” Ð½Ð¾Ð²Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾: file_column_name -> internal_key.
    """
    current = load_custom_mapping()
    col_lower = file_column_name.lower().strip()
    
    if internal_key == 'IGNORE':
        ignored = current.get('IGNORE', [])
        if col_lower not in ignored:
            ignored.append(col_lower)
            current['IGNORE'] = ignored
    else:
        aliases = current.get(internal_key, [])
        if col_lower not in aliases:
            aliases.append(col_lower)
            current[internal_key] = aliases

    with open(MAPPING_FILE, 'w', encoding='utf-8') as f:
        json.dump(current, f, ensure_ascii=False, indent=2)
    logger.info(f"Mapping saved: {col_lower} -> {internal_key}")

# ==============================================================================
# ðŸ” Ð”Ð•Ð¢Ð•ÐšÐ¦Ð†Ð¯ ÐšÐžÐ›ÐžÐÐžÐš
# ==============================================================================

def detect_columns(df: pd.DataFrame) -> Tuple[Dict[str, str], List[str]]:
    """
    ÐœÐ°Ð¿Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ñƒ Ð½Ð° Ð²Ð½ÑƒÑ‚Ñ€Ñ–ÑˆÐ½Ñ– Ð½Ð°Ð·Ð²Ð¸.
    """
    detected = {}
    df_cols_lower = {str(c).lower().strip(): c for c in df.columns}
    
    custom_map = load_custom_mapping()
    
    combined_mapping = DEFAULT_MAPPING.copy()
    for k, v in custom_map.items():
        if k != 'IGNORE':
            combined_mapping[k] = combined_mapping.get(k, []) + v
    
    ignored_list = IGNORED_COLUMNS + custom_map.get('IGNORE', [])

    used_file_cols = set()
    
    for key, aliases in combined_mapping.items():
        found = None
        for alias in aliases:
            if alias in df_cols_lower:
                found = df_cols_lower[alias]
                used_file_cols.add(found)
                break
        detected[key] = found

    unknown = []
    for col in df.columns:
        c_low = str(col).lower().strip()
        if (col not in used_file_cols and 
            c_low not in ignored_list and 
            not str(col).startswith("Unnamed")):
            unknown.append(str(col))

    return detected, unknown

# ==============================================================================
# ðŸ”¨ Ð’ÐÐ›Ð†Ð”ÐÐ¦Ð†Ð¯ Ð¢Ð Ð•ÐšÐ¡Ð¢Ð ÐÐšÐ¦Ð†Ð¯
# ==============================================================================

def extract_article_and_name(text: str) -> Tuple[str, str]:
    """
    Ð Ð¾Ð·Ð´Ñ–Ð»ÑÑ” '12345678 - ÐÐ°Ð·Ð²Ð° Ñ‚Ð¾Ð²Ð°Ñ€Ñƒ' Ð½Ð° Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ» Ñ– Ð½Ð°Ð·Ð²Ñƒ.
    """
    if not text or pd.isna(text):
        return "", ""
    
    s = str(text).strip()
    
    # ðŸ”¥ Ð’Ð˜ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð•ÐºÑ€Ð°Ð½ÑƒÐ²Ð°Ð½Ð½Ñ Ð´ÐµÑ„Ñ–ÑÑƒ [\s\-\â€“\â€”]
    m = re.match(r"^(\d{8})[\s\-\â€“\â€”]+(.+)$", s)
    if m:
        return m.group(1), m.group(2).strip()
        
    # Ð¡Ð¿Ñ€Ð¾Ð±Ð° 2: ÐŸÑ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ð±Ñ–Ð»
    m = re.match(r"^(\d{8})\s+(.+)$", s)
    if m:
        return m.group(1), m.group(2).strip()
        
    # Ð¯ÐºÑ‰Ð¾ Ð½Ðµ Ð²Ð´Ð°Ð»Ð¾ÑÑ Ñ€Ð¾Ð·Ð´Ñ–Ð»Ð¸Ñ‚Ð¸ - Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ”Ð¼Ð¾ ÑÐº Ñ” (Ð¼Ð¾Ð¶Ð»Ð¸Ð²Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ» Ð² Ñ–Ð½ÑˆÑ–Ð¹ ÐºÐ¾Ð»Ð¾Ð½Ñ†Ñ–)
    return "", s

def validate_article(val) -> Tuple[bool, Optional[str]]:
    s = str(val).strip()
    if not s: return False, "ÐŸÑƒÑÑ‚Ð¾"
    if not s.isdigit(): return False, "ÐÐµ Ñ†Ð¸Ñ„Ñ€Ð¸"
    if len(s) != 8: return False, "ÐÐµ 8 Ñ†Ð¸Ñ„Ñ€"
    return True, None

# ==============================================================================
# ðŸ“Š ÐžÐ‘Ð ÐžÐ‘ÐšÐ DATAFRAME
# ==============================================================================

def process_import_dataframe(df: pd.DataFrame, custom_map=None) -> Tuple[pd.DataFrame, ImportValidation]:
    """
    ÐŸÐµÑ€ÐµÑ‚Ð²Ð¾Ñ€ÑŽÑ” Ð²Ñ…Ñ–Ð´Ð½Ð¸Ð¹ DataFrame Ñƒ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚.
    """
    col_map, _ = detect_columns(df)
    if custom_map:
        col_map.update(custom_map)

    errors = []
    warnings = []
    rows = []
    
    # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð¼Ñ–Ð½Ñ–Ð¼ÑƒÐ¼Ñƒ: Ð¿Ð¾Ð²Ð¸Ð½Ð½Ð° Ð±ÑƒÑ‚Ð¸ Ñ…Ð¾Ñ‡Ð° Ð± ÐšÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ
    if not col_map.get("quantity"):
        return df, ImportValidation(
            False, ["ÐÐµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ ÐºÐ¾Ð»Ð¾Ð½ÐºÑƒ 'ÐšÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ'"], [], len(df), 0
        )

    for idx, row in df.iterrows():
        rid = idx + 2
        try:
            art, name = "", ""
            
            # --- 1. ÐÐ Ð¢Ð˜ÐšÐ£Ð› Ð¢Ð ÐÐÐ—Ð’Ð ---
            # Ð’Ð°Ñ€Ñ–Ð°Ð½Ñ‚ Ð: Ð„ Ð¾ÐºÑ€ÐµÐ¼Ñ– ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
            if col_map.get("article") and col_map.get("name"):
                art = str(row[col_map["article"]]).strip()
                name = str(row[col_map["name"]]).strip()
            
            # Ð’Ð°Ñ€Ñ–Ð°Ð½Ñ‚ Ð‘: Ð„ Ñ‚Ñ–Ð»ÑŒÐºÐ¸ ÐÐ°Ð·Ð²Ð° (Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ» Ð²ÑÐµÑ€ÐµÐ´Ð¸Ð½Ñ–)
            elif col_map.get("name") and not col_map.get("article"):
                art, name = extract_article_and_name(row[col_map["name"]])
                
            # Ð’Ð°Ñ€Ñ–Ð°Ð½Ñ‚ Ð’: Ð„ Ñ‚Ñ–Ð»ÑŒÐºÐ¸ ÐÑ€Ñ‚Ð¸ÐºÑƒÐ»
            elif col_map.get("article"):
                art = str(row[col_map["article"]]).strip()

            # Ð’Ð°Ð»Ñ–Ð´Ð°Ñ†Ñ–Ñ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñƒ
            valid, _ = validate_article(art)
            if not valid:
                # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°Ñ”Ð¼Ð¾ Ñ€ÑÐ´ÐºÐ¸ Ð±ÐµÐ· Ð²Ð°Ð»Ñ–Ð´Ð½Ð¾Ð³Ð¾ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»Ñƒ (Ð¿Ñ–Ð´ÑÑƒÐ¼ÐºÐ¸, ÑÐ¼Ñ–Ñ‚Ñ‚Ñ)
                continue 

            # --- 2. ÐšÐ†Ð›Ð¬ÐšÐ†Ð¡Ð¢Ð¬ (ÐžÐ±Ð¾Ð²'ÑÐ·ÐºÐ¾Ð²Ðµ) ---
            qty_raw = str(row[col_map["quantity"]]).replace(",", ".").replace(" ", "").replace("\xa0", "")
            try:
                qty = float(qty_raw)
            except:
                errors.append(f"Ð ÑÐ´ {rid} (ÐÑ€Ñ‚ {art}): Ð¿Ð¾Ð¼Ð¸Ð»ÐºÐ° ÐºÑ–Ð»ÑŒÐºÐ¾ÑÑ‚Ñ– '{qty_raw}'")
                continue

            # --- 3. Ð†ÐÐ¨Ð† ÐŸÐžÐ›Ð¯ (ÐÐµÐ¾Ð±Ð¾Ð²'ÑÐ·ÐºÐ¾Ð²Ñ– -> None) ---
            dept = None
            if col_map.get("department"):
                try: dept = int(float(str(row[col_map["department"]])))
                except: pass
            
            grp = None
            if col_map.get("group"):
                grp = str(row[col_map["group"]]).strip()

            sum_val = None
            price = None
            if col_map.get("sum"):
                try:
                    sum_val = float(str(row[col_map["sum"]]).replace(",", ".").replace(" ", "").replace("\xa0", ""))
                    if qty > 0:
                        price = round(sum_val / qty, 2)
                except: pass
            
            mnth = None
            if col_map.get("months_no_movement"):
                try: mnth = int(float(str(row[col_map["months_no_movement"]])))
                except: pass

            rows.append({
                "Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»": art,
                "Ð½Ð°Ð·Ð²Ð°": name,
                "Ð²Ñ–Ð´Ð´Ñ–Ð»": dept,
                "Ð³Ñ€ÑƒÐ¿Ð°": grp,
                "ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ": qty,
                "Ñ†Ñ–Ð½Ð°": price,
                "ÑÑƒÐ¼Ð°_Ð·Ð°Ð»Ð¸ÑˆÐºÑƒ": sum_val,
                "Ð¼Ñ–ÑÑÑ†Ñ–_Ð±ÐµÐ·_Ñ€ÑƒÑ…Ñƒ": mnth
            })

        except Exception as e:
            errors.append(f"Ð ÑÐ´ {rid}: {e}")

    processed_df = pd.DataFrame(rows)
    
    return processed_df, ImportValidation(
        is_valid=len(rows) > 0,
        errors=errors,
        warnings=warnings,
        total_rows=len(df),
        valid_rows=len(rows)
    )

def generate_import_preview(df: pd.DataFrame) -> ImportPreview:
    cmap, unk = detect_columns(df)
    return ImportPreview(
        columns_detected=cmap,
        unknown_columns=unk,
        sample_rows=df.head(3),
        stats={"total_rows": len(df), "columns_count": len(df.columns)},
        header_row_index=0
    )